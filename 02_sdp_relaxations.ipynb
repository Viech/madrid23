{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semidefinite relaxations of discrete problems\n",
    "\n",
    "Semidefinite programs often play a role in the design of approximation\n",
    "algorithms for hard discrete problems. This notebook shows a famous example of\n",
    "this and introduces some useful properties of semidefinite matrices.\n",
    "\n",
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy import spatial\n",
    "from plotly import offline, graph_objects as go\n",
    "\n",
    "COORDS_KEY = \"coords\"\n",
    "WEIGHT_KEY = \"weight\"\n",
    "LAYOUT = dict(\n",
    "    scene=dict(xaxis_visible=False, yaxis_visible=False, zaxis_visible=False),\n",
    "    yaxis=dict(scaleanchor=\"x1\"), margin=dict(l=10, r=10, t=10, b=10),\n",
    "    paper_bgcolor=\"white\", plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    ")\n",
    "\n",
    "offline.init_notebook_mode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 2: Maximum cut in undirected graphs\n",
    "\n",
    "### Setting\n",
    "\n",
    "A *cut* in a weighted undirected graph $I = (V, E)$ with edge weights $w_e \\geq\n",
    "0$ is a partition of the vertex set $V$ into two disjoint subsets $C$ and $V\n",
    "\\setminus C$. In the *minimum* cut problem, one considers in addition two\n",
    "terminal vertices $s \\neq t \\in V$ and requires that $s \\in C$ and $t \\in V\n",
    "\\setminus C$. Here the goal is to choose $C$ such that the total weight of\n",
    "edges leaving $C$, formally\n",
    "\n",
    "$$\n",
    "    w(C) = \\sum_{e \\in E, \\; |e \\cap C| = 1} w_e\n",
    "$$\n",
    "\n",
    "is minimized. It is well known that such a minimum cut [can be computed\n",
    "efficiently via linear\n",
    "programming](https://en.wikipedia.org/wiki/Max-flow_min-cut_theorem).\n",
    "\n",
    "The *maximum* cut problem, on the other end, is an $\\mathsf{NP}$-complete\n",
    "problem, so no polynomial time algorithm can exist for it unless $\\mathsf{P} =\n",
    "\\mathsf{NP}$. Here, we do not consider terminal vertices&mdash;$C \\subseteq V$\n",
    "may be chosen arbitrarily&mdash;and we want to maximize $w(C)$. This problem\n",
    "pops up, for instance, in [statistical\n",
    "mechanics](https://en.wikipedia.org/wiki/Ising_model#Connection_to_graph_maximum_cut).\n",
    "\n",
    "Let's generate a random graph to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40  # number of vertices\n",
    "\n",
    "def make_weighted_graph(points):\n",
    "    \"\"\"Create a weighted graph from a Delaunay triangulation of 2D points.\n",
    "\n",
    "    Edge weights are set to the Euclidean distances between the points.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    triangulation = spatial.Delaunay(points)\n",
    "\n",
    "    for path in triangulation.simplices:\n",
    "        nx.add_path(G, path)\n",
    "\n",
    "    for v in G.nodes:\n",
    "        G.nodes[v][COORDS_KEY] = triangulation.points[v, :]\n",
    "\n",
    "    nx.set_edge_attributes(\n",
    "        G,\n",
    "        {\n",
    "            (u, v): np.linalg.norm(\n",
    "                G.nodes[u][COORDS_KEY] - G.nodes[v][COORDS_KEY]\n",
    "            )\n",
    "            for u, v in G.edges\n",
    "        },\n",
    "        WEIGHT_KEY,\n",
    "    )\n",
    "\n",
    "    return G\n",
    "\n",
    "def vert_data(G, select=None):\n",
    "    \"\"\"Report vertex coordinates of a graph for plotting with plotly.\"\"\"\n",
    "    x_vec, y_vec = [], []\n",
    "\n",
    "    for u, d in G.nodes(data=True):\n",
    "        if select is None or select[u]:\n",
    "            x, y = d[COORDS_KEY]\n",
    "            x_vec.append(x)\n",
    "            y_vec.append(y)\n",
    "\n",
    "    return dict(x=x_vec, y=y_vec)\n",
    "\n",
    "def edge_data(G, x=None, only_cut=None):\n",
    "    \"\"\"Report edge coordinates of a graph for plotting with plotly.\"\"\"\n",
    "    x_seq, y_seq = [], []\n",
    "\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        if x is None or (x[u]*x[v] > 0) ^ only_cut:\n",
    "            ux, uy = G.nodes[u][COORDS_KEY]\n",
    "            vx, vy = G.nodes[v][COORDS_KEY]\n",
    "            x_seq.extend((ux, vx, None))\n",
    "            y_seq.extend((uy, vy, None))\n",
    "\n",
    "    return dict(x=x_seq, y=y_seq)\n",
    "\n",
    "# Create a graph from random points.\n",
    "points = np.random.random((n, 2))\n",
    "G = make_weighted_graph(points)\n",
    "\n",
    "# Plot the graph.\n",
    "traces = [\n",
    "    go.Scatter(\n",
    "        mode=\"lines+markers\", **edge_data(G),\n",
    "        line_color=\"lightgray\", marker_color=\"black\"),\n",
    "]\n",
    "go.Figure(traces, LAYOUT).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, the weight of an edge is set to the Euclidean distance\n",
    "between its endpoints. We are looking to partition the vertices such that the\n",
    "weight of edges between the partitions is as large as possible.\n",
    "\n",
    "### The problem\n",
    "\n",
    "Since we cannot hope to find an efficient algorithm that always produces an\n",
    "optimum solution to the maximum cut problem, we will have to do with an\n",
    "approximation. An approximation algorithm has ratio $\\rho$ if for every problem\n",
    "instance $I$ (here $I = G$), the value of the solution returned by the\n",
    "algorithm, $\\operatorname{ALG}(I)$, is always within a factor of $\\rho$ within\n",
    "the optimum value $\\operatorname{OPT}(I)$, formally $\\rho = \\sup_I\n",
    "\\frac{\\operatorname{ALG}(I)}{\\operatorname{OPT}(I)} \\geq 1$ for minimization\n",
    "problems and $\\rho = \\inf_I \\frac{\\operatorname{ALG}(I)}{\\operatorname{OPT}(I)}\n",
    "\\leq 1$ for maximization problems. In the following we will recover a famous\n",
    "(randomized) $0.878$-approximation algorithm ([Goemans and Williamson,\n",
    "1995](#references)) for the maximum cut problem that is based on semidefinite\n",
    "programming!\n",
    "\n",
    "First, we need to introduce the graph Laplacian. For an undirected graph $G =\n",
    "(V, E)$ with $V = \\{1, \\ldots, n\\}$ and edge weights $w_e$, this is the\n",
    "symmetric matrix $L \\in \\mathbb{S}^n$ with\n",
    "\n",
    "$$\n",
    "    L_{i,j} = \\begin{cases}\n",
    "        \\sum_{k = 1}^n w_{\\{i, k\\}}, &\\text{if}~i = j, \\\\\n",
    "        -w_{\\{i,j\\}}, &\\text{if}~i \\neq j,\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "where we assume $w_{\\{i,j\\}} = 0$ for $i, j \\in V$ with $\\{i,j\\} \\not\\in E$.\n",
    "Informally, $L$ has the weighted vertex degrees on the diagonal and the\n",
    "negative edge weights on the off-diagonals (whose indices $i, j$ correspond to\n",
    "edges $\\{i, j\\}$).\n",
    "\n",
    "Equipped with the Laplacian matrix $L$, the maximum cut problem can be stated as\n",
    "follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{maximize} ~&~ \\frac{1}{4} x^T L x \\\\\n",
    "    \\text{where}\n",
    "    ~&~ x \\in \\{-1, 1\\}^n \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In this form, $x_i = 1$ represents $i \\in C$ and $x_i = -1$ represents $x_i \\in\n",
    "V \\setminus C$.\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Bonus task 2.1:** Show that, indeed, $w(C) = \\frac{1}{4} x^T L x$ for $x$\n",
    "   defined as above.\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Hint</summary>\n",
    "\n",
    "   - Write $L$ as $L = D + H$ where $D$ is a diagonal matrix and where $H$ is a\n",
    "     \"hollow\" matrix with zeros on the diagonal.\n",
    "   - Compute $x^T D x$ and $x^T H x$, then compare their sum with $w(C)$\n",
    "\n",
    "   </details>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Task 2.2:** Convince your group partner(s) of the following statements\n",
    "   concerning the matrix $X = xx^T$:\n",
    "\n",
    "   1. $X$ is symmetric,\n",
    "   2. $\\operatorname{rank}(X) = 1$,\n",
    "   3. $\\operatorname{diag}(X) = \\mathbf{1}$, and\n",
    "   4. $X$ is positive semidefinite, written $X \\succeq 0$.\n",
    "\n",
    "</div>\n",
    "\n",
    "One can show also the following: If an $n \\times n$ matrix $X$ fulfills\n",
    "properties (1) to (3), then it can be written as $X = xx^T$ with $x \\in \\{-1,\n",
    "1\\}^n$. Indeed, property (4) already follows from (1) and (2), so there is some\n",
    "overlap in the above statements.\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Task 2.3:** Rewrite the solution to Task 2.1 to use the matrix $X$ instead\n",
    "   of $x$.\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Recommended hint</summary>\n",
    "\n",
    "   - Since $w(C)$ is a scalar, $w(C) = \\operatorname{Tr}(w(C))$, where\n",
    "     $\\operatorname{Tr}$ denotes the [trace of a square\n",
    "     matrix](https://en.wikipedia.org/wiki/Trace_(linear_algebra)).\n",
    "   - Use suited properties of the trace.\n",
    "\n",
    "   </details>\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Notation options</summary>\n",
    "\n",
    "   - It is $\\langle A, B \\rangle = \\operatorname{Tr}(A^T B) =\n",
    "     \\operatorname{Tr}(A B^T)$ where $\\langle \\cdot, \\cdot \\rangle$ denotes the\n",
    "     [Frobenius inner\n",
    "     product](https://en.wikipedia.org/wiki/Frobenius_inner_product) for real\n",
    "     matrices. Both the inner product and the trace notation are common in the\n",
    "     literature on semidefinite programming.\n",
    "\n",
    "   </details>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    w(C) = w(X) = {\\color{red}{\\text{[some function of $L$ and $X$]}}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have rewritten the convex objective $w(C)$ as a linear objective.\n",
    "(Recall that we are *maximizing*, so a convex objective is not helpful!) Of\n",
    "course, this does not come for free, as we have implicitly added the non-convex\n",
    "constraint $X = xx^T$, which in turn is equivalent to the four properties\n",
    "outlined in Task 2.2. Let's write down what we have so far:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "\n",
    "$$\n",
    "\\providecommand{\\TODO}{}\\renewcommand{\\TODO}{{\\color{red}{[\\ldots]}}}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{maximize} ~&~ w(X) \\tag{1} \\\\\n",
    "    \\text{subject to}\n",
    "    ~&~ \\operatorname{rank}(X) = 1, \\tag{1a} \\\\\n",
    "    ~&~ \\operatorname{diag}(X) = \\mathbf{1}, \\tag{1b} \\\\\n",
    "    ~&~ X \\succeq 0, \\tag{1c} \\\\\n",
    "    \\text{where}\n",
    "    ~&~ X \\in \\mathbf{S}^n. \\tag{1d} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the objective and constraint (1b) are linear while constraint\n",
    "(1c) (together with (1d)) enforces membership in the cone of symmetric positive\n",
    "semidefinite matrices ($S^n_+$); so this part of the problem is both convex and\n",
    "computationally tractable. The hard part is constraint (1a). If we *just leave\n",
    "it out*, we obtain a relaxation of the original problem:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "\n",
    "$$\n",
    "\\providecommand{\\TODO}{}\\renewcommand{\\TODO}{{\\color{red}{[\\ldots]}}}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{maximize} ~&~ w(X) \\tag{2} \\\\\n",
    "    \\text{subject to}\n",
    "    ~&~ \\operatorname{diag}(X) = \\mathbf{1}, \\\\\n",
    "    ~&~ X \\succeq 0, \\\\\n",
    "    \\text{where}\n",
    "    ~&~ X \\in \\mathbf{S}^n.\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Task 2.4:** Discuss the following in your group:\n",
    "\n",
    "   1. Will the optimum objective value of problem (2) be greater or lower than\n",
    "      that of problem (1)?\n",
    "   2. Can you explain (or sketch) why the constraint $X \\succeq 0$ may be a\n",
    "      useful addition to problem (2) even though we argued earlier that it is\n",
    "      redundant for problem (1)?\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "   **Take a pause.** We'll discuss solutions before we move on.\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Task 2.5:** Complete the code below to solve problem (2).\n",
    "\n",
    "</div>\n",
    "\n",
    "You'll need some of the following:\n",
    "\n",
    "| on paper | in picos |\n",
    "| --- | --- |\n",
    "| $X \\in \\mathcal{S}^n$ | `X = pc.SymmetricVariable(\"X\", n)` |\n",
    "| $A \\preceq B$ | `A << B` |\n",
    "| $A \\succeq B$ | `A >> B` |\n",
    "| $\\langle A, B \\rangle$ | `(A\\|B)` |\n",
    "| $\\operatorname{Tr}(A)$ | `A.tr` |\n",
    "| $\\operatorname{diag}(A)$ | `A.maindiag` |\n",
    "\n",
    "Here $\\operatorname{Tr}(A) = \\sum_i A_{ii}$ denotes the trace and\n",
    "$\\operatorname{diag}(A)$ the main diagonal (as a column vector) of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import picos as pc\n",
    "\n",
    "# Load the graph Laplacian of G.\n",
    "L = pc.Constant(\"L\", nx.laplacian_matrix(G))\n",
    "\n",
    "# TODO: Implement problem (2).\n",
    "# X = \n",
    "# P = \n",
    "",
    "\n",
    "# TODO: Solve it. (Reduce the number of vertices if this takes too long.)\n",
    "",
    "\n",
    "# Show solution details.\n",
    "objective_value = P.value\n",
    "total_weight = L.tr.value / 2\n",
    "fraction = objective_value/total_weight\n",
    "print(\n",
    "    f\"Objective value:   {objective_value:6.3f}\\n\"\n",
    "    f\"Total edge weight: {total_weight:6.3f}\\n\"\n",
    "    f\"Fraction:          {fraction:6.3f}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above shows an objective value and a fraction of around $0.75$ to $0.8$,\n",
    "then we have solved the relaxation! But what can we do with the solution? After\n",
    "all, we would like to make a binary decision for each of the $n$ vertices but\n",
    "instead we got a continuous $n \\times n$ matrix.\n",
    "\n",
    "If that matrix happens to be of rank one, then we would have solved problem (1).\n",
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_X = np.linalg.matrix_rank(X.np, tol=1e-5)\n",
    "print(f\"rank(X) = {rank_X} (n = {n})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most likely: no luck! So how does the algorithm of Goemans and Williamson\n",
    "proceed from here?\n",
    "\n",
    "First, it uses a [Cholesky\n",
    "decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition): Every\n",
    "symmetric positive semidefinite matrix can be written as an outer product of a\n",
    "lower-triangular matrix with itself, that is\n",
    "\n",
    "$$\n",
    "    X = TT^T.\n",
    "$$\n",
    "\n",
    "Had we solved problem (1), then it would be $t_i t_j^T = X_{ij} = x_i x_j$ for\n",
    "all $i$ and $j$, where $t_k$ is the $k$-th **row** of $T$. This would tell us\n",
    "how to partition the nodes optimally: exactly the vertices with $t_i t_j^T = 1$\n",
    "go into the same partition. But as we solved the relaxed problem (2), $t_i t_j^T\n",
    "\\in [-1, 1]$ is not necessarily integral and only provides a hint as to whether\n",
    "vertices $i$ and $j$ should be in the same partition ($t_i t_j^T$ close to $1$)\n",
    "or not ($t_i t_j^T$ close to $-1$). So let us try to \"round\" the continuous row\n",
    "vector $t_i$ to some $x_i \\in \\{-1, 1\\}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Bonus task 2.6:** Discuss the following:\n",
    "\n",
    "   1. What is $\\lVert t_i \\rVert$?\n",
    "   2. Given a random column vector $v \\in \\mathbb{R}^n$ with $\\lVert v \\rVert =\n",
    "      1$, what would be a natural way to map $t_i$ to $1$ or $-1$? (Make a sketch\n",
    "      in 2D.)\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Hint</summary>\n",
    "\n",
    "   - Use $\\operatorname{diag}(X) = \\mathbf{1}$.\n",
    "\n",
    "   </details>\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Solution</summary>\n",
    "\n",
    "   - The algorithm uses $v$ to put $i \\in C$ if and only if $t_i v \\geq 1$. In\n",
    "     other words, $x_i = \\operatorname{sign}(t_i v)$ and $x =\n",
    "     \\operatorname{sign}(T v)$.\n",
    "\n",
    "   </details>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Task 2.7:** Complete the code below to obtain a partitioning $x \\in \\{-1,\n",
    "   1\\}^n$.\n",
    "\n",
    "<details>\n",
    "<summary style='display: list-item'>Documentation for NumPy functions</summary>\n",
    "\n",
    "You can use\n",
    "\n",
    "- [np.linalg.cholesky](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cholesky.html),\n",
    "- [np.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html),\n",
    "- [np.random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html),\n",
    "- [np.sign](https://numpy.org/doc/stable/reference/generated/numpy.sign.html).\n",
    "\n",
    "Recall that NumPy uses `@` for matrix-matrix and matrix-vector multiplication.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary style='display: list-item'>Hint on generating unit norm vectors uniformly at random</summary>\n",
    "\n",
    "- The easiest way to pick a point on a $(d-1)$-dimensional unit sphere (embedded\n",
    "  in $\\mathbb{R}^d$) uniformly at random is to sample the $d$-dimensional\n",
    "  standard normal distribution and normalize the resulting vector.\n",
    "- (Here, normalization turns out to be unnecessary but doesn't hurt either.)\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_plot_partition(G, x):\n",
    "    assert len(x) == n\n",
    "    assert np.allclose(np.abs(x), 1)\n",
    "\n",
    "    inC = vert_data(G, x > 0)\n",
    "    VmC = vert_data(G, x < 0)\n",
    "    cut = edge_data(G, x, True)\n",
    "    unc = edge_data(G, x, False)\n",
    "\n",
    "    traces = [\n",
    "        go.Scatter(mode=\"lines\", **cut, line_color=\"purple\", name=\"cut\"),\n",
    "        go.Scatter(mode=\"lines\", **unc, line_color=\"lightgray\", name=\"uncut\"),\n",
    "        go.Scatter(mode=\"markers\", **inC, marker_color=\"red\", name=\"C\"),\n",
    "        go.Scatter(mode=\"markers\", **VmC, marker_color=\"blue\", name=\"V \\\\ C\"),\n",
    "    ]\n",
    "\n",
    "    go.Figure(traces, LAYOUT).show()\n",
    "\n",
    "    wC = (x.T * L * x / 4).value\n",
    "    wE = L.tr.value / 2\n",
    "\n",
    "    print(\n",
    "        f\"n:             {n:6d}\\n\"\n",
    "        f\"|C|:           {sum(x > 0):6d}\\n\"\n",
    "        f\"|V \\\\ C|:      {sum(x < 0):7d}\\n\"\n",
    "        f\"w(C):          {wC:6.3f}\\n\"\n",
    "        f\"w(E):          {wE:6.3f}\\n\"\n",
    "        f\"w(C) / w(E):   {wC/wE:6.3f}\\n\"\n",
    "        f\"{'-'*21}\\n\"\n",
    "        f\"Relaxation:    {fraction:6.3f}\\n\"\n",
    "        f\"Approx. ratio: {(wC/wE)/fraction:6.3f}\"\n",
    "    )\n",
    "\n",
    "# TODO: Perform a Cholesky decomposition T of X.\n",
    "# T = \n",
    "\n",
    "# TODO: Generate a vector v with unit norm uniformly at random.\n",
    "# v = \n",
    "\n",
    "# TODO: Generate an n-vector x with entries -1 and 1 from T and v.\n",
    "# x = \n",
    "\n",
    "# Show the partition.\n",
    "analyze_and_plot_partition(G, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected approximation ratio is at least $0.878$. If all went well, your\n",
    "program should exceed this at least half of the time. (Rerun the last cell to\n",
    "get a new random rounding and the whole notebook for a new instance.)\n",
    "\n",
    "In practice, one could repeat the random projection until the bound is exceeded,\n",
    "which leads to guaranteed performance but a randomized runtime with at most two\n",
    "rounds in expectation.\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Bonus task 2.8:** Prove that the expected approximation ratio\n",
    "   $\\frac{\\mathbb{E}(w(C))}{\\operatorname{OPT}}$ is at least $0.878$ by filling\n",
    "   in the gaps. You will need the following:\n",
    "\n",
    "   1. Another relaxation of MAXCUT (closely related to the Cholesky\n",
    "      decomposition argument) is the following program:\n",
    "\n",
    "      $$\n",
    "        \\begin{align*}\n",
    "            \\text{maximize} ~&~\n",
    "              \\sum_{\\{i, j\\} \\in E} w_{\\{i,j\\}}\n",
    "              \\frac{1 - \\langle t_i, t_j \\rangle}{2} \\tag{3} \\\\\n",
    "            \\text{subject to}\n",
    "            ~&~ \\lVert t_i \\rVert = 1 \\quad \\text{for}~i \\in \\{1, \\ldots, n\\} \\\\\n",
    "            \\text{where}\n",
    "            ~&~ t_i \\in \\mathbb{R}^n \\quad \\text{for}~i \\in \\{1, \\ldots, n\\} \\\\\n",
    "        \\end{align*}\n",
    "      $$\n",
    "\n",
    "      (To see that this is a relaxation, take $t_i = x_i e_1$ for all $i$.) We\n",
    "      can denote its optimum value as $\\operatorname{REL}$. Since this is a\n",
    "      relaxation, $\\operatorname{OPT} \\leq \\operatorname{REL}$.\n",
    "   2. The angle between two unit vectors $t_i$ and $t_j$ in radians is\n",
    "      $\\arccos{\\langle t_i, t_j \\rangle}$. Use this to describe the probability\n",
    "      $\\mathbb{P}(x_i x_j = 1)$. (Make use of your sketch from task 2.6!)\n",
    "   3. It is $$\\frac{\\arccos{\\tau}}{\\pi} \\geq 0.878 \\frac{1 - \\tau}{2}$$ for all\n",
    "      $\\tau \\in [-1, 1]$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "\n",
    "$$\n",
    "\\providecommand{\\TODO}{}\\renewcommand{\\TODO}{{\\color{red}{[\\ldots]}}}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}(w(C))\n",
    "    &= \\sum_{\\{i, j\\} \\in E} w_{\\{i, j\\}} \\mathbb{P}(x_i x_j = 1) \\\\\n",
    "    &= \\sum_{\\{i, j\\} \\in E} w_{\\{i, j\\}} \\TODO \\\\\n",
    "    &\\geq 0.878 \\left( \\sum_{\\{i, j\\} \\in E} w_{\\{i, j\\}} \\TODO \\right) \\\\\n",
    "    &= 0.878 \\cdot \\TODO \\\\\n",
    "    &\\geq 0.878 \\cdot \\operatorname{OPT}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "- A quantum analog of the maximum cut problem is studied in a recent preprint by\n",
    "  King ([2022](#references)) and in earlier work, e.g. by Gharibian and Parekh\n",
    "  ([2019](#references)).\n",
    "\n",
    "## References\n",
    "\n",
    "- Gharibian and Parekh. 2019. Almost optimal classical approximation algorithms\n",
    "  for a quantum generalization of Max-Cut. arXiv preprint arXiv:1909.08846.\n",
    "  https://doi.org/10.48550/arXiv.1909.08846\n",
    "- Michel X. Goemans and David P. Williamson. 1995. Improved approximation\n",
    "  algorithms for maximum cut and satisfiability problems using semidefinite\n",
    "  programming. *J. ACM* 42, 6 (Nov. 1995), 1115–1145.\n",
    "  https://doi.org/10.1145/227683.227684\n",
    "- Robbie King. 2022. An Improved Approximation Algorithm for Quantum Max-Cut.\n",
    "  arXiv preprint arXiv:2209.02589. https://doi.org/10.48550/arXiv.2209.02589"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
